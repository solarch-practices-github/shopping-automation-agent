---
title: Models
description: Choose and configure language models for your agents
---

import { Code } from '@astrojs/starlight/components';
import modelCustomProviderExample from '../../../../../examples/docs/models/customProviders.ts?raw';
import setDefaultOpenAIKeyExample from '../../../../../examples/docs/config/setDefaultOpenAIKey.ts?raw';
import modelSettingsExample from '../../../../../examples/docs/models/modelSettings.ts?raw';
import promptIdExample from '../../../../../examples/basic/prompt-id.ts?raw';
import agentWithModelExample from '../../../../../examples/docs/models/agentWithModel.ts?raw';
import runnerWithModelExample from '../../../../../examples/docs/models/runnerWithModel.ts?raw';
import gpt5DefaultModelSettingsExample from '../../../../../examples/docs/models/gpt5DefaultModelSettings.ts?raw';
import setTracingExportApiKeyExample from '../../../../../examples/docs/config/setTracingExportApiKey.ts?raw';

Every Agent ultimately calls an LLM. The SDK abstracts models behind two lightweight interfaces:

- [`Model`](/openai-agents-js/openai/agents/interfaces/model) – knows how to make _one_ request against a specific API.
- [`ModelProvider`](/openai-agents-js/openai/agents/interfaces/modelprovider) – resolves human‑readable model **names** (e.g. `'gpt‑5.2'`) to `Model` instances.

In day‑to‑day work you normally only interact with model **names** and occasionally `ModelSettings`.

<Code
  lang="typescript"
  code={agentWithModelExample}
  title="Specifying a model per‑agent"
/>

## Default model

When you don't specify a model when initializing an `Agent`, the default model will be used. The default is currently [`gpt-4.1`](https://platform.openai.com/docs/models/gpt-4.1) for compatibility and low latency. If you have access, we recommend setting your agents to [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) for higher quality while keeping explicit `modelSettings`.

If you want to switch to other models like [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2), there are two ways to configure your agents.

First, if you want to consistently use a specific model for all agents that do not set a custom model, set the `OPENAI_DEFAULT_MODEL` environment variable before running your agents.

```bash
export OPENAI_DEFAULT_MODEL=gpt-5.2
node my-awesome-agent.js
```

Second, you can set a default model for a `Runner` instance. If you don't set a model for an agent, this `Runner`'s default model will be used.

<Code
  lang="typescript"
  code={runnerWithModelExample}
  title="Set a default model for a Runner"
/>

### GPT-5.x models

When you use any GPT-5.x model such as [`gpt-5.2`](https://platform.openai.com/docs/models/gpt-5.2) in this way, the SDK applies default `modelSettings`. It sets the ones that work the best for most use cases. To adjust the reasoning effort for the default model, pass your own `modelSettings`:

<Code
  lang="typescript"
  code={gpt5DefaultModelSettingsExample}
  title="Customize GPT-5 default settings"
/>

For lower latency, using `reasoning.effort: "none"` with `gpt-5.2` is recommended. The gpt-4.1 family (including mini and nano variants) also remains a solid choice for building interactive agent apps.

### Non-GPT-5 models

If you pass a non–GPT-5 model name without custom `modelSettings`, the SDK reverts to generic `modelSettings` compatible with any model.

---

## The OpenAI provider

The default `ModelProvider` resolves names using the OpenAI APIs. It supports two distinct
endpoints:

| API              | Usage                                                             | Call `setOpenAIAPI()`                   |
| ---------------- | ----------------------------------------------------------------- | --------------------------------------- |
| Chat Completions | Standard chat & function calls                                    | `setOpenAIAPI('chat_completions')`      |
| Responses        | New streaming‑first generative API (tool calls, flexible outputs) | `setOpenAIAPI('responses')` _(default)_ |

### Authentication

<Code
  lang="typescript"
  code={setDefaultOpenAIKeyExample}
  title="Set default OpenAI key"
/>

You can also plug your own `OpenAI` client via `setDefaultOpenAIClient(client)` if you need
custom networking settings.

---

## ModelSettings

`ModelSettings` mirrors the OpenAI parameters but is provider‑agnostic.

| Field                  | Type                                                            | Notes                                                                     |
| ---------------------- | --------------------------------------------------------------- | ------------------------------------------------------------------------- |
| `temperature`          | `number`                                                        | Creativity vs. determinism.                                               |
| `topP`                 | `number`                                                        | Nucleus sampling.                                                         |
| `frequencyPenalty`     | `number`                                                        | Penalise repeated tokens.                                                 |
| `presencePenalty`      | `number`                                                        | Encourage new tokens.                                                     |
| `toolChoice`           | `'auto' \| 'required' \| 'none' \| string`                      | See [forcing tool use](/openai-agents-js/guides/agents#forcing-tool-use). |
| `parallelToolCalls`    | `boolean`                                                       | Allow parallel function calls where supported.                            |
| `truncation`           | `'auto' \| 'disabled'`                                          | Token truncation strategy.                                                |
| `maxTokens`            | `number`                                                        | Maximum tokens in the response.                                           |
| `store`                | `boolean`                                                       | Persist the response for retrieval / RAG workflows.                       |
| `promptCacheRetention` | `'in-memory' \| '24h' \| null`                                  | Controls provider prompt-cache retention when supported.                  |
| `reasoning.effort`     | `'none' \| 'minimal' \| 'low' \| 'medium' \| 'high' \| 'xhigh'` | Reasoning effort for gpt-5.x models.                                      |
| `reasoning.summary`    | `'auto' \| 'concise' \| 'detailed'`                             | Controls how much reasoning summary the model returns.                    |
| `text.verbosity`       | `'low' \| 'medium' \| 'high'`                                   | Text verbosity for gpt-5.x etc.                                           |
| `providerData`         | `Record<string, any>`                                           | Provider-specific passthrough options forwarded to the underlying model.  |

Attach settings at either level:

<Code lang="typescript" code={modelSettingsExample} title="Model settings" />

`Runner`‑level settings override any conflicting per‑agent settings.

---

## Prompt

Agents can be configured with a `prompt` parameter, indicating a server-stored
prompt configuration that should be used to control the Agent's behavior. Currently,
this option is only supported when you use the OpenAI
[Responses API](https://platform.openai.com/docs/api-reference/responses).

| Field       | Type     | Notes                                                                                                                                  |
| ----------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| `promptId`  | `string` | Unique identifier for a prompt.                                                                                                        |
| `version`   | `string` | Version of the prompt you wish to use.                                                                                                 |
| `variables` | `object` | A key/value pair of variables to substitute into the prompt. Values can be strings or content input types like text, images, or files. |

<Code lang="typescript" code={promptIdExample} title="Agent with prompt" />

Any additional agent configuration, like tools or instructions, will override the
values you may have configured in your stored prompt.

---

## Custom model providers

Implementing your own provider is straightforward – implement `ModelProvider` and `Model` and
pass the provider to the `Runner` constructor:

<Code
  lang="typescript"
  code={modelCustomProviderExample}
  title="Minimal custom provider"
/>

If you want a ready-made adapter for non-OpenAI models, see [Using any model with Vercel's AI SDK](/openai-agents-js/extensions/ai-sdk).

---

## Tracing exporter

When using the OpenAI provider you can opt‑in to automatic trace export by providing your API
key:

<Code
  lang="typescript"
  code={setTracingExportApiKeyExample}
  title="Tracing exporter"
/>

This sends traces to the [OpenAI dashboard](https://platform.openai.com/traces) where you can
inspect the complete execution graph of your workflow.

---

## Next steps

- Explore [running agents](/openai-agents-js/guides/running-agents).
- Give your models super‑powers with [tools](/openai-agents-js/guides/tools).
- Add [guardrails](/openai-agents-js/guides/guardrails) or [tracing](/openai-agents-js/guides/tracing) as needed.
