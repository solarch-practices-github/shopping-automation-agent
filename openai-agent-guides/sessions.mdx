---
title: Sessions
description: Persist multi-turn conversation history so agents can resume context across runs.
---

import { Code } from '@astrojs/starlight/components';
import sessionsQuickstart from '../../../../../examples/docs/sessions/basicSession.ts?raw';
import manageHistory from '../../../../../examples/docs/sessions/manageHistory.ts?raw';
import customSession from '../../../../../examples/docs/sessions/customSession.ts?raw';
import sessionInputCallback from '../../../../../examples/docs/sessions/sessionInputCallback.ts?raw';
import responsesCompactionSession from '../../../../../examples/docs/sessions/responsesCompactionSession.ts?raw';
import manualCompactionSession from '../../../../../examples/docs/sessions/responsesCompactionManualSession.ts?raw';

Sessions give the Agents SDK a **persistent memory layer**. Provide any object that implements the `Session` interface to `Runner.run`, and the SDK handles the rest. When a session is present, the runner automatically:

1. Fetches previously stored conversation items and prepends them to the next turn.
2. Persists new user input and assistant output after each run completes.
3. Keeps the session available for future turns, whether you call the runner with new user text or resume from an interrupted `RunState`.

This removes the need to manually call `toInputList()` or stitch history between turns. The TypeScript SDK ships with two implementations: `OpenAIConversationsSession` for the Conversations API and `MemorySession`, which is intended for local development. Because they share the `Session` interface, you can plug in your own storage backend. For inspiration beyond the Conversations API, explore the sample session backends under `examples/memory/` (Prisma, file-backed, and more). When you use an OpenAI Responses model, wrap any session with `OpenAIResponsesCompactionSession` to automatically shrink stored transcripts via [`responses.compact`](https://platform.openai.com/docs/api-reference/responses/compact).

> Tip: To run the `OpenAIConversationsSession` examples on this page, set the `OPENAI_API_KEY` environment variable (or provide an `apiKey` when constructing the session) so the SDK can call the Conversations API.

---

## Quick start

Use `OpenAIConversationsSession` to sync memory with the [Conversations API](https://platform.openai.com/docs/api-reference/conversations), or swap in any other `Session` implementation.

<Code
  lang="typescript"
  code={sessionsQuickstart}
  title="Use the Conversations API as session memory"
/>

Reusing the same session instance ensures the agent receives the full conversation history before every turn and automatically persists new items. Switching to a different `Session` implementation requires no other code changes.

`OpenAIConversationsSession` constructor options:

| Option           | Type     | Notes                                                          |
| ---------------- | -------- | -------------------------------------------------------------- |
| `conversationId` | `string` | Reuse an existing conversation instead of creating one lazily. |
| `client`         | `OpenAI` | Pass a preconfigured OpenAI client.                            |
| `apiKey`         | `string` | API key used when creating an internal OpenAI client.          |
| `baseURL`        | `string` | Base URL for OpenAI-compatible endpoints.                      |
| `organization`   | `string` | OpenAI organization ID for requests.                           |
| `project`        | `string` | OpenAI project ID for requests.                                |

---

## How the runner uses sessions

- **Before each run** it retrieves the session history, merges it with the new turn's input, and passes the combined list to your agent.
- **After a non-streaming run** one call to `session.addItems()` persists both the original user input and the model outputs from the latest turn.
- **For streaming runs** it writes the user input first and appends streamed outputs once the turn completes.
- **When resuming from `RunResult.state`** (for approvals or other interruptions) keep passing the same `session`. The resumed turn is added to memory without re-preparing the input.

---

## Inspecting and editing history

Sessions expose simple CRUD helpers so you can build "undo", "clear chat", or audit features.

<Code
  lang="typescript"
  code={manageHistory}
  title="Read and edit stored items"
/>

`session.getItems()` returns the stored `AgentInputItem[]`. Call `popItem()` to remove the last entry—useful for user corrections before you rerun the agent.

---

## Bring your own storage

Implement the `Session` interface to back memory with Redis, DynamoDB, SQLite, or another datastore. Only five asynchronous methods are required.

<Code
  lang="typescript"
  code={customSession}
  title="Custom in-memory session implementation"
/>

Custom sessions let you enforce retention policies, add encryption, or attach metadata to each conversation turn before persisting it.

---

## Control how history and new items merge

When you pass an array of `AgentInputItem`s as the run input, provide a `sessionInputCallback` to merge them with stored history deterministically. The runner loads the existing history, calls your callback **before the model invocation**, and hands the returned array to the model as the turn’s complete input. This hook is ideal for trimming old items, deduplicating tool results, or highlighting only the context you want the model to see.

<Code
  lang="typescript"
  code={sessionInputCallback}
  title="Truncate history with sessionInputCallback"
/>

For string inputs the runner merges history automatically, so the callback is optional.

---

## Handling approvals and resumable runs

Human-in-the-loop flows often pause a run to wait for approval:

```typescript
const result = await runner.run(agent, 'Search the itinerary', {
  session,
  stream: true,
});

if (result.requiresApproval) {
  // ... collect user feedback, then resume the agent in a later turn
  const continuation = await runner.run(agent, result.state, { session });
  console.log(continuation.finalOutput);
}
```

When you resume from a previous `RunState`, the new turn is appended to the same memory record to preserve a single conversation history. Human-in-the-loop (HITL) flows stay fully compatible—approval checkpoints still round-trip through `RunState` while the session keeps the transcript complete.

---

## Compact OpenAI Responses history automatically

`OpenAIResponsesCompactionSession` decorates any `Session` and relies on the OpenAI Responses API to keep transcripts short. After each persisted turn the runner passes the latest `responseId` into `runCompaction`, which calls `responses.compact` when your decision hook returns true. The default trigger compacts once at least 10 non-user items have accumulated; override `shouldTriggerCompaction` to base the decision on token counts or custom heuristics. The decorator clears and rewrites the underlying session with the compacted output, so avoid pairing it with `OpenAIConversationsSession`, which uses a different server-managed history flow.

<Code
  lang="typescript"
  code={responsesCompactionSession}
  title="Decorate a session with OpenAIResponsesCompactionSession"
/>

`OpenAIResponsesCompactionSession` constructor options:

| Option                    | Type                                          | Notes                                                                                                    |
| ------------------------- | --------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| `client`                  | `OpenAI`                                      | OpenAI client used for `responses.compact`.                                                              |
| `underlyingSession`       | `Session`                                     | Backing session store to clear/rewrite with compacted items (must not be `OpenAIConversationsSession`).  |
| `model`                   | `OpenAI.ResponsesModel`                       | Model used for compaction requests.                                                                      |
| `compactionMode`          | `'auto' \| 'previous_response_id' \| 'input'` | Controls whether compaction uses server response chaining or local input items.                          |
| `shouldTriggerCompaction` | `(context) => boolean \| Promise<boolean>`    | Custom trigger hook based on `responseId`, `compactionMode`, candidate items, and current session items. |

`runCompaction(args)` options:

| Option           | Type                                          | Notes                                                             |
| ---------------- | --------------------------------------------- | ----------------------------------------------------------------- |
| `responseId`     | `string`                                      | Latest Responses API response id for `previous_response_id` mode. |
| `compactionMode` | `'auto' \| 'previous_response_id' \| 'input'` | Optional per-call override of the configured mode.                |
| `store`          | `boolean`                                     | Indicates whether the last run stored server state.               |
| `force`          | `boolean`                                     | Bypass `shouldTriggerCompaction` and compact immediately.         |

### Manual compaction for low-latency streaming

Compaction clears and rewrites the underlying session, so the SDK waits for it before resolving a streaming run. If compaction is heavy, `result.completed` can stay pending for a few seconds after the last output token. For low-latency streaming or faster turn-taking, disable auto-compaction and call `runCompaction` yourself between turns (or during idle time).

<Code
  lang="typescript"
  code={manualCompactionSession}
  title="Disable auto-compaction and compact between turns"
/>

You can call `runCompaction({ force: true })` at any time to shrink history before archiving or handoff. Enable debug logs with `DEBUG=openai-agents:openai:compaction` to trace compaction decisions.
